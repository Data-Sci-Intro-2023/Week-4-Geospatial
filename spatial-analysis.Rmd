---
title: "Spatial Data Analysis"
author: "Caitlin Mothes"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In the first lesson this week you were exposed to various databases you can pull spatial data from and worked through importing, wrangling, and saving those spatial data types. Today we are going to use those data sets to perform a range of spatial analyses.

You have briefly used the `sf` and `terra` packages so far in this course, but today we will be exploring them much more in depth using the wide range of spatial analysis operations they provide.

You shouldn't need to install any new packages for today:

```{r}
source("setup.R")
```

## Load in spatial data

To carry out today's lesson you will need to read in the data you saved from yesterday

```{r}
#load in all your vector data
load("{whatever you called this file}.RData")

#read in the elevation and landcover rasters
landcover <- terra::rast("{name of file}.tif")

elevation <- terra::rast("{name of file}.tif")
```

## Distance Calculations

We're going to start off today with some distance calculations. Using our species occurrence data, say we want to compare each species average distance to rivers.

Throughout today we are going to be mapping our spatial data to quickly inspect it and get a visual of the data's extent and characteristics, so lets set our tmap mode to interactive.

```{r}
tmap_mode("view")
```

Quick view of all our points, colored by species:

```{r}
qtm(occ, symbols.col = "Species")
```

Now, for each species we want to find their average distance to rivers and roads. This involves point to line distance calculations, which we can perform with the `sf` package.

Before performing any spatial operations, remember all of our spatial objects must be in the same CRS.

CHECKPOINT

Using what you learned in week one, check the CRS of the occurrences and rivers, and perform a spatial transformation if needed.

```{r}
st_crs(rivers)
st_crs(roads)
st_crs(occ)
```

EXERCISE

Our occurrence data set covers all of Colorado, but rivers are only for Larimer County. So, we have to first filter our points to Larimer County. Explore the use of `st_filter()` and use it to filter points that are found within the Larimer County polygon (from your `counties` object).

```{r}
counties <- st_read("data/CO_counties.shp")

occ_larimer <- st_filter(occ, counties[counties$NAME == "Larimer",])

qtm(occ_larimer)
```

Great, now we just have species occurrences within Larimer County.

Now for each point we want to calculate its distance to the nearest river. The most efficient way is to first find the nearest line feature for each point. We can do this with the `st_nearest_feature()` function.

This function returns the index values (row number) of the river feature in the `rivers` spatial data frame that is closest in distance to each point. Here we are saving these index values in a new column of our Larimer occurrences that we will use later to calculate distances.

```{r}
occ_larimer$nearest_river <- st_nearest_feature(occ_larimer, rivers)
```

Now, for each point we can use the `st_distance()` function to calculate the distance to the nearest river feature, using the index value in our new "nearest_river" column. Adding `by_element = TRUE` is necessary to tell the function to perform the distance calculations by element (row), which we will fill into a new column "river_dist_m".

```{r}
occ_larimer$river_dist_m <- st_distance(occ_larimer, rivers[occ_larimer$nearest_river,], by_element = TRUE)
```

Notice that the new column is more than just a numeric class, but a "units" class, specifying that the values are in meters.

```{r}
str(occ_larimer)
```

EXERCISE

Cool, now we have the distance to the nearest river (in meters) for each individual species occurrence. Now say we want the average distance for each species. We can do some data wrangling to get these values using the `dplyr()` package. Using the pipe `%>%` operator again, we perform a chain of operations on the data frame. `group_by()` specifies that all following operations should be performed individually by a grouping variable, in this case we want to apply operations on each individual species. Next `summarise()` calculates a new column "river_dist" that is the average (per species) river distance, where we have to convert our 'units' column to numeric to perform the `mean()` function.

```{r}
#| message: false
#| warning: false
occ_larimer %>% 
  group_by(Species) %>% 
  summarise(river_dist = (mean(as.numeric(river_dist_m))))

```

Let's make a quick bar plot to visually compare. We can pipe this output into a `ggplot()` object, specifying which variables go on the x and y axes within `aes()` and then we want to fill the bar color by species. `geom_col()` returns a barplot where the heights of the bars represent values in the data (in our case species average distance to a river).

```{r}
occ_larimer %>% 
  group_by(Species) %>% 
  summarise(river_dist = (mean(as.numeric(river_dist_m)))) %>% 
  ggplot(aes(Species, river_dist, fill = Species)) +
  geom_col()
```

EXERCISE

Create this plot:

```{r}
occ_larimer %>% 
  group_by(Species) %>% 
  summarise(river_dist = mean(as.numeric(river_dist_m))) %>% 
  ggplot(aes(Species, river_dist, fill = Species)) +
  geom_col() +
  labs(y = "Average distance to nearest river (m)") +
  theme(legend.position = "none") #removes the legend
        
        
```

## Buffers

Alternatively, say you want to know what percentage of species' occurrences (points) were found within a certain distance of a river (calculated buffer).

To do this we could add a buffer around our line features and filter the points that fall within that buffer zone. For this example let's say we are interested in the 100 m buffer zone around rivers. However, if you try this you'll notice this operation takes quite a while.

```{r}
#| eval: false
river_buffer <- st_buffer(rivers, dist = 100)
```

Instead, a more efficient way would be to make a 100 m buffer around each point, and see how many of those buffers intersect with a river.

```{r}
occ_buffer <- st_buffer(occ_larimer, dist = 100)

```

Still takes a little bit of run time, but much faster than buffering each line feature. Our `occ_buffer` object is now a spatial polygon data frame, where each feature is an occurrence buffer with 100 m radius.

## Spatial Intersect

We can conduct spatial intersect operations using the function `st_intersects()`. This function checks if each individual buffer intersects with a river, and if so it returns an index value (row number) for each river feature it intersects. This function returns a list object for each buffer polygon, that will be empty if there are no intersections. We will add this as a column to our buffer data set, and then create a binary yes/no river intersection column based on those results (is the list empty or not).

```{r}
river_intersections <- st_intersects(occ_buffer, rivers)
```

If we inspect this object, we see it is a list of the same length as our `occ_buffer` object, where each list element is either empty (no intersections) or a list of index numbers for the river features that do intersect that buffer.

EXERCISE

Create a new column that returns TRUE/FALSE if the buffer intersects with a river. (Hint: make use of the `length()` function..we aren't interested at this point in how many rivers are within 100m of a species, just whether or not there was a river within the buffer or not).

```{r}
occ_buffer$river_100m <- lengths(river_intersections) > 0
```

Then calculate what percentage of occurrences are within 100 m of a river for each species using `dplyr` operations. However, the below code does not work. Why not? There is one line of code you need to add to the pipe operations for this to work, what is it?

```{r}
occ_buffer %>% 
  #st_drop_geometry() %>% #we use this function to treat the object as a dataframe
  group_by(Species) %>% 
  summarise(total_occ = n(), percent_river = (sum(river_100m == TRUE)/total_occ)*100)
```

## Spatial/Nonspatial Joins 

*Spatial join snotel sites to watersheds!*

We already used this tool a little already, but we can use `left_join()` to add a bunch more attributes to our counties shapefile. First lets add our census data, which was collected at the county level and saved as a csv.

```{r}
census <- read_csv("data/census_data.csv") %>% 
  dplyr::select(-NAME)
```

We first want to remove the 'NAME' column, because it is slightly different than the 'NAME' column in our counties data and therefore will not join properly. We can instead join by matching 'GEOID', which is a unique numeric ID given to each county.

```{r}
counties_attr <- counties_attr %>% 
  left_join(census, by = "GEOID")
```

Lastly, lets join our species occurrence data set. Say we want to know how many species occurrences (at least of our three study species) are found in each county. Here we go back to the `st_intersects()` function we used before, which returns a list of all spatial elements that intersect with the spatial features of interest, in this case how many occurrence points intersect with each county polygon. We then nest this within `lengths()` which will return the number of intersecting features (occurrences) for each county, and we put those values as a new column called "species_count".

```{r}
counties_attr$species_count <- lengths(st_intersects(counties_attr, occ))
```

Now we have a bunch of information tied to our county shapefile, we can explore it spatially and comparatively.

```{r}
qtm(counties_attr, fill = "species_count")
```

Use `ggplot2` to visualize the relationship between different county attributes.

```{r}
counties_attr %>% 
  st_drop_geometry() %>% 
  ggplot(aes(total_pop, species_count)) +
  geom_point() +
  stat_smooth()
```

```{r}
# urban area related to co_born
counties_attr %>% 
  st_drop_geometry() %>% 
  ggplot(aes(urban_coverage, species_count)) +
  geom_point() +
  stat_smooth()
```

## Raster Reclassification

So far we've dealt with a bunch of vector data and associated analyses with the `sf` package. Now lets work through some raster data analysis using the `terra` package.

```{r}
qtm(landcover)
```

This land cover data set includes attributes (land cover classes) associated with raster values. We can quickly view the frequency of each land cover type with the `freq()` function.

```{r}
freq(landcover)
```

Use `ggplot2` to turn this into a bar chart

```{r}
freq(landcover) %>% 
  ggplot(aes(reorder(value, count), count)) +
  labs(x = "") +
  geom_col() +
  coord_flip() # switch the axes to better view land cover class names
```

Say we want to explore some habitat characteristics of our species of interest, and we are specifically interested in forest cover. Our first step is to create a new raster layer from our land cover layer representing percent forest cover. This will involve multiple operations, include raster reclassification and focal statistics. Specifically, say we want to calculate the average percentage of forest cover and urbanization within a 9x9 pixel moving window (remember since rasters are made up of pixels, the distances we use are dependent on the resolution of the raster).

CHECKIN

Approximately what 'buffer' area are we calculate percent forest by using a 9x9 moving window?

First lets reclassify our land cover raster, creating a new raster representing just forest/non-forest pixels.

Since rasters are technically matrices, we can index and change values using matrix operations. Given this particular raster uses character names associated with values, we can index by those names.

```{r}
#first assign landcover to a new object name so we can manipulate it while keeping the origian
forest <- landcover

#where the raster equals any of the forest categories, set that value to 1
forest[forest %in% c("Deciduous Forest", "Evergreen Forest", "Mixed Forest")] <- 1

#SPELLING IS IMPORTANT

#now set all non forest pixels to NA
forest[forest != 1] <- NA
```

Lets look at our new forest layer

```{r}
qtm(forest)
```

## Focal Statistics

Now we are going to perform focal statistics, which is a spatial operation that calculates new values for each cell based on a specified moving window. For this example we are going to calculate within a 9x9km moving window (since our pixel resolution is 1km). We supply this to the `w =` argument as a matrix, where the first value is the weight of each pixel, and the second two are the number of rows and columns. Second we use the "sum" function, since each forest pixel has a value of 1 we will get the total number of forest pixels within the moving window, and then later divide the values by the total number of pixels in the window (81) to get the percentage. The final raster values will represent for each pixel the surrounding forest percentage (within \~4.5 km radius).

```{r}
forest_pct <- terra::focal(forest, w=matrix(1,9,9), fun = "sum", na.rm = TRUE)
```

```{r}
forest_pct <- forest_pct/81
```

```{r}
plot(forest_pct)
```

Next, we wanted to know the percent forest cover associated with each species occurrence. Since we are now working with multiple spatial objects, we have to first check they are all in the same CRS and if not transform the data before any spatial operations.

```{r}
crs(forest_pct)

st_crs(occ)
```

Looks like the raster layer is in a different CRS. Let's reproject this so we can use it with our vector data (which are all in NAD83). We can project raster data to a new CRS with the `project()` function from `terra`.

One thing to note is that while `terra` does work with vector data, it wants them to be in a special format called a `SpatVector` , instead of an `sf` object. Luckily they have made it quick and easy to convert between the data formats using the `vect()` function, so we just need to remember to nest our `sf` objects within that function when using them in `terra` functions.

```{r}
forest_pct <- project(forest_pct, vect(occ))
```

## Raster Extract

Now we can use the `extract()` function to extract the raster pixel value at each occurrence.

```{r}
terra::extract(forest_pct, vect(occ))
```

Notice that this returns a 2 column data frame, with an ID for each feature (occurrence) and the extracted raster value in the second column. We want to add these raster values as a new column to our occurrence data set, so we need to index just this second column of the `extract()` output.

```{r}
occ$forest_pct <- terra::extract(forest_pct, vect(occ))[,2]
```

Now let's do the same extract method with our elevation raster, pulling the elevation value at each species occurrence.

Again, we need to first project the raster to the CRS of the occurrence data set.

```{r}
elevation <-  terra::project(elevation, vect(occ))
```

Now we can do the same extraction as with the forest raster, putting the values in a new 'elevation' column

```{r}
occ$elevation <- terra::extract(elevation, vect(occ))[,2]
```

Let's do some data frame operations to calculate and compare average forest cover across species

```{r}
occ %>% 
  group_by(Species) %>% 
  summarise(avg_forest_pct = mean(forest_pct, na.rm = TRUE))
```

```{r}
occ %>% 
  group_by(Species) %>% 
  summarise(avg_forest_pct = mean(forest_pct, na.rm = TRUE)) %>% 
  ggplot(aes(Species, avg_forest_pct, fill = Species))+
  geom_col() +
  theme(legend.position = "bottom")
```

Looks like elk are associated with the most forested habitats. We can also view the spread of values with a box plot.

```{r}
ggplot(occ, aes(Species, forest_pct)) +
  geom_boxplot()
  
```

That's one way to use the `extract()` function. We can also extract raster values within polygons, and supply a function to summarize those raster values.

Say we wanted to know the most common land cover type in each county. We can use `extract()` with the function 'modal' to return the most common type for each polygon, and we will add this as a new column.

Again we need to project our land cover raster.

```{r}
landcover_prj <- project(landcover, vect(counties))
```

```{r}
terra::extract(landcover_prj, vect(counties), fun = "modal")

```

This works similar to what we ran with the occurrence dataset...however we notice that it returns the raw raster values instead of the named land cover classes (which we want). This raster behaves such that values are named factors, and we can pull this metadata with the `cats()` function.

```{r}
cats(landcover)[[1]]
```

This returns a single element list that is a data frame with a bunch of information. We just want to keep 'value' and 'NLCD Land Cover Class", and remove all the empty classes. We have to first index the first element of the list to operate on just the data frame, then we can apply some `dplyr` functions.

```{r}
nlcd_classes <- cats(landcover)[[1]] %>% 
  dplyr::select("value", nlcd_class = "NLCD Land Cover Class") %>% 
  filter(nlcd_class != "")
```

Cool, now we can tie this to our counties data frame once we have the most common land cover value calculated with `extract`

```{r}
counties$common_landcover <- terra::extract(landcover_prj, vect(counties), fun = "modal")[,2]
```

This join is different from our others because the variable we want to join by has a different column name in each data set, but we can specify this within the `by =` argument like this:

```{r}
counties <- counties %>% 
  left_join(nlcd_classes, by = c("common_landcover" = "value"))
```

We can now get some summary statistics on the most common land cover types per county.

```{r}
counties %>% 
  st_drop_geometry() %>% #for ggplot, don't need geometry
  group_by(nlcd_class) %>% 
  summarise(n = n()) %>% 
  ggplot(aes(nlcd_class, n, fill = nlcd_class)) +
  geom_col()+
  theme(legend.position = "none")+
  coord_flip()
```

And map it out by county:

```{r}
tm_shape(counties)+
  tm_polygons("nlcd_class")
```

## Additional Exercises

Using watershed data from lesson 1 this week

1.  For a single county {suggest which one} that consists of multiple watersheds, calculate the percent area of that county that each watershed covers. Which watershed covers the largest area of {} county?

    Note that since we are wanting to calculate the actual area of intersection (not just whether or not an intersection exists as before) you will need to use the `st_intersection()` function.

2.  Calculate the most common landcover type in each watershed and create a map of it.
